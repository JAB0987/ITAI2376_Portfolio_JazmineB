# AI Learning Portfolio – ITAI2376  
**By Jazmine Brown | Spring 2025**

Welcome to my AI learning journey for Deep Learning (ITAI2376)! This GitHub repository showcases my assignments, labs, code, and reflections from the course, demonstrating my growth in building and understanding deep learning models. Through hands-on projects and creative assignments, I explored neural networks, computer vision, NLP, Transformers, and generative AI, gaining practical skills in PyTorch, TensorFlow, and more.


---

##  Portfolio Highlights

- Built Deep Learning Models: Developed models for image classification (MNIST, VGG16), sentiment analysis (Amazon reviews), and generative tasks (Fashion-MNIST) using PyTorch, TensorFlow/Keras, and Hugging Face Transformers.

- Mastered Advanced Topics: Explored RNNs, CNNs, Transformers, and Diffusion Models, applying them to real-world problems like NLP and image generation.

- Debugging & Optimization: Overcame challenges like library errors, shape mismatches, and memory constraints through problem-solving and hyperparameter tuning.

- Creative Applications: Connected AI concepts to linguistic relativity (Arrival analysis) and intuitive analogies (backpropagation as cooking).


---
## Repository Structure
This repository is organized into folders for each module, plus additional folders for resources and the final presentation. Below is the structure with key files:

| Folder | Description | Key Files |
|--------|-------------|-----------|
| Module_2_DeepLearningTools | JAX vs. NumPy comparison, VGG16 image classification lab | JAX_vs_NumPy.md, VGG16_Lab_Reflection.md |
| Module_3_NeuralNetworkBasics | Neural Network Zoo presentation, PyTorch labs (tensors, MLP, text processing) | Neural_Zoo_Presentation.md, Lab_01_PyTorch.md, Lab_02_MLP_FashionMNIST.md, Lab_03_Text_NN.md |
| Module_4_CNN | MNIST CNN lab, backpropagation presentation | CNN_MNIST_Lab.md, Backpropagation_Presentation.md, mnist_cnn.py |
| Module_5_SequenceModeling | RNN for Amazon reviews, *Arrival* NLP analysis | RNN_Amazon_Reviews_Lab.md, Arrival_NLP_Assignment.md, rnn_model.py |
| Module_6_Transformers | Fine-tuning DistilBERT for sentiment analysis | BERT_Sentiment_Lab.md, bert_finetune.py |
| Module_9_GenerativeAI | Diffusion Models with U-Net for Fashion-MNIST | Diffusion_Models_Summary.md, unet_diffusion.py |
| Module_11_AgentPlanning | Ai Research Assistant| FN_Report, FN_Code, FN_Demo |
| Resources | External articles, videos, and documentation | Resources.md |
| Presentation | Final PowerPoint presentation (PDF) | Pf_JazmineBrown_ITAI2376.pdf |

## Modules & Topics Covered
The table below summarizes the modules, key topics, tools used, and highlights from my work:

| Module | Topic | Tools | Highlights |
|:-------|:------|:------|:-----------|
| M2 | Deep Learning Environment | NumPy, JAX, PyTorch | Compared NumPy (CPU-based) vs. JAX (GPU/TPU, 100x speedups). Classified Pexels image with VGG16 (breakwater, 29.5%). Learned transfer learning, CNN complexity (138M parameters). |
| M3 | Neural Networks | PyTorch | Presented Neural Network Zoo (e.g., RNNs as octopuses). Built MLP for Fashion-MNIST, text NN for pet adoption. Mastered tensors, dropout, backpropagation. Resolved NameError in Lab 03. |
| M4 | CNNs & Backpropagation | TensorFlow/Keras | Achieved 98.96% accuracy on MNIST CNN. Explained backpropagation via cooking analogy. Fixed shape mismatch with Adam optimizer. Experimented with kernel sizes, filters. |
| M5 | NLP & Linguistic Relativity | PyTorch, GloVe, AWS SageMaker | Built RNN for Amazon review sentiment (79.01% accuracy). Analyzed *Arrival* for NLP challenges (e.g., ambiguity). Mastered tokenization, embeddings, sequential processing. |
| M6 | Transformers | Hugging Face Transformers, PyTorch | Fine-tuned DistilBERT for sentiment analysis (~89% accuracy). Managed memory by freezing layers. Learned transfer learning’s efficiency for NLP. |
| M9 | Generative AI | PyTorch, CLIP | Trained U-Net diffusion model for Fashion-MNIST (MSE loss ~0.105). Evaluated outputs with CLIP scores. Proposed CLIP-guided improvements for image quality. |





---

##  Tools & Libraries Used

- Python: Core programming language, used in Jupyter Notebooks via Google Colab.
- PyTorch: For tensor operations, MLP, RNN, and diffusion models.
- TensorFlow/Keras: For CNN implementation (MNIST).
- Hugging Face Transformers: For DistilBERT fine-tuning.
- NumPy & JAX: For numerical computing and high-performance ML.
- Matplotlib & Seaborn: For visualizing model performance (e.g., accuracy curves).
- AWS SageMaker: For scalable RNN training in Module 5.
- Other: GloVe embeddings, DistilBertTokenizerFast, CLIP for evaluation.

---
## Key Resources

- Hugging Face Transformers Documentation: Guided BERT fine-tuning and tokenization.
- BERT Paper (arXiv:1810.04805): Deepened understanding of bidirectional context in NLP.
- PyTorch Documentation: Supported tensor operations, model building, and optimization.
- AWS SageMaker: Enabled scalable training for RNN lab.
- Image Source: Pexels.com for VGG16 lab image.

---
##  Reflection

This course transformed my understanding of AI from theoretical concepts to practical applications. Through hands-on labs, I built models for computer vision (MNIST, VGG16), NLP (Amazon reviews, BERT), and generative tasks (diffusion models), mastering tools like PyTorch and TensorFlow. Debugging challenges—like library errors, shape mismatches, and memory constraints—taught me resilience and problem-solving. Creative assignments, such as analyzing Arrival for NLP or comparing backpropagation to cooking, deepened my ability to connect AI to real-world problems. I now feel confident applying AI to domains like education (e.g., STEM tutoring, inspired by group projects) and aspire to explore advanced models like LSTMs and generative AI further.

Quote: “The best way to learn AI is to build with it.” – Jazmine Brown

## Attribution
- Group Contributions: Assignments like Neural Network Zoo, backpropagation, and Arrival analysis were collaborative efforts by Group 2Deep2Learn.
- Images: VGG16 lab image sourced from Pexels.com (free license).
- Resources: Cited articles, documentation, and papers (e.g., Hugging Face, arXiv) are linked in Resources.md.

For questions or feedback, contact me via GitHub or course platform.

